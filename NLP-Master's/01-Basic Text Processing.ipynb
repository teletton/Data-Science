{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture I - Basic Text Processing, Data sources and Corpora\n",
    "\n",
    "This is my first notebook from the master's class Natural Language Processing. It is not all the same, this is more like summary and I will add some additional stuff.\n",
    "\n",
    "\n",
    "## Short Introduction to NLTK\n",
    "\n",
    "NLTK is a python library for text processing.\n",
    "\n",
    "First let's import the nltk libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tavchija/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/tavchija/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /home/tavchija/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package reuters to /home/tavchija/nltk_data...\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/tavchija/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/tavchija/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package tagsets to /home/tavchija/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"gutenberg\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"tagsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of tokenization of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Jhon', 'works', 'at', 'OBI', '.']\n",
      "POS tagging: [('Jhon', 'NNP'), ('works', 'VBZ'), ('at', 'IN'), ('OBI', 'NNP'), ('.', '.')]\n",
      "Light parsing: (S (PERSON Jhon/NNP) works/VBZ at/IN (ORGANIZATION OBI/NNP) ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "text = \"Jhon works at OBI.\"\n",
    "\n",
    "#Morphology Level\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "#Syntax Level\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(\"POS tagging:\", tagged_tokens)\n",
    "\n",
    "#Semantics Level\n",
    "ner_tree = ne_chunk(tagged_tokens)\n",
    "print(\"Light parsing:\", ner_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Text object as example. Sometimes it can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to Monday:\n",
      "april march friday february january\n",
      "Common contexts to a list of words August, June\n",
      "and_and in_the in_and last_the on_the last_when between_and in_to\n",
      "last_that for_shipment in_because ended_shr from_to since_and and_at\n",
      "last_to in_last for_to in_u in_they\n",
      "Contexts of a word Monday\n",
      "Displaying 25 of 240 matches:\n",
      "said . Trade Minister Saleh said on Monday that Indonesia , as the world ' s s\n",
      "Reuters to clarify his statement on Monday in which he said the pact should be\n",
      " the 11 - member CPA which began on Monday . They said producers agreed that c\n",
      "ief Burkhard Junger was arrested on Monday on suspicion of embezzlement and of\n",
      "ween one and 1 . 25 billion dlrs on Monday and Tuesday . The spokesman said Mo\n",
      "ay and Tuesday . The spokesman said Monday ' s float included 500 mln dlrs in \n",
      "s ranged from minus 500 mln dlrs on Monday , when cash letter errors at two ea\n",
      " a deficit on Thursday , Friday and Monday but held excess reserves on the fin\n",
      "ed temporary reserves indirectly on Monday via two billion dlrs of customer re\n",
      "ssets to secure the judgment . Last Monday , the Supreme Court overturned a de\n",
      " Central Bank were higher than last Monday ' s offering , the bank said . The \n",
      " to 98 . 39844 from 98 . 45313 last Monday . Like - dated interbank deposits w\n",
      "e under fresh scrutiny from today ( Monday ), with activity in the European an\n",
      "ose from an unfavorable ruling last Monday by the U . S . Supreme Court in Tex\n",
      "could ,\" Elton said . But following Monday ' s Supreme Court ruling , Texaco '\n",
      "ould come under fresh scrutiny from Monday , with activity in the European and\n",
      "and Trade Minister Rha Woong Bae on Monday to discuss opening South Korean mar\n",
      "oleum Intelligence Weekly , in this Monday ' s edition , said negotiations are\n",
      "nced May 20 , rose another 50 cents Monday to 8 . 375 a share in over - the - \n",
      "ment Marketing were up 12 . 5 cents Monday to 9 . 50 . FIRST UNION & lt ; FUNC\n",
      "tinue to raise U . S . oil prices . Monday , after Texaco confirmed that the p\n",
      "tinue to raise U . S . oil prices . Monday , after Texaco confirmed that the p\n",
      "s . The offer , which will begin on Monday and ends June 30 , requires that th\n",
      "ts two - week public flotation last Monday . The government has carried out ei\n",
      " a U . S . Court in Orlando , Fla . Monday . The company has sought a declarat\n"
     ]
    }
   ],
   "source": [
    "from nltk import Text \n",
    "from nltk.corpus import reuters\n",
    "\n",
    "text = Text(reuters.words())\n",
    "\n",
    "print(\"Similar words to Monday:\")\n",
    "text.similar('Monday', 5)\n",
    "\n",
    "print(\"Common contexts to a list of words August, June\")\n",
    "text.common_contexts(['August', 'June'])\n",
    "\n",
    "print(\"Contexts of a word Monday\")\n",
    "text.concordance('Monday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of work with n-grams (bigrams, trigrams) and collocations extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 50 bigrams according to PMI: [('DU', 'PONT'), ('Keng', 'Yaik'), ('Kwik', 'Save'), ('Nihon', 'Keizai'), ('corenes', 'pora'), ('fluidized', 'bed'), ('Akbar', 'Hashemi'), ('Constructions', 'Telephoniques'), ('Elevator', 'Mij'), ('Entre', 'Rios'), ('Graan', 'Elevator'), ('JIM', 'WALTER'), ('Taikoo', 'Shing'), ('der', 'Vorm'), ('di', 'Clemente'), ('Borrowing', 'Requirement'), ('FOOTE', 'MINERAL'), ('Hawker', 'Siddeley'), ('JARDINE', 'MATHESON'), ('PRORATION', 'FACTOR'), ('Wildlife', 'Refuge'), ('Kohlberg', 'Kravis'), ('Almir', 'Pazzionotto'), ('Bankhaus', 'Centrale'), ('Corpus', 'Christi'), ('Kuala', 'Lumpur'), ('Maple', 'Leaf'), ('Stats', 'Oljeselskap'), ('Zoete', 'Wedd'), ('Neutral', 'Zone'), ('Tadashi', 'Kuranari'), ('Drawing', 'Rights'), ('EASTMAN', 'KODAK'), ('Martinez', 'Cuenca'), ('Mathematical', 'Applications'), ('Townsend', 'Thoresen'), ('Sector', 'Borrowing'), ('Hashemi', 'Rafsanjani'), ('Hossein', 'Mousavi'), ('Kitty', 'Hawk'), ('SLAUGHTER', 'GUESSTIMATES'), ('Task', 'Force'), ('Tender', 'Loving'), ('WELLS', 'FARGO'), ('ad', 'hoc'), ('mechanically', 'separated'), ('bleached', 'deodorised'), ('Alejandro', 'Martinez'), ('Het', 'Comite'), ('Paz', 'Estenssoro')]\n",
      "Best 50 bigrams according to PMI: [('Graan', 'Elevator', 'Mij'), ('Sector', 'Borrowing', 'Requirement'), ('Akbar', 'Hashemi', 'Rafsanjani'), ('Lim', 'Keng', 'Yaik'), ('Alejandro', 'Martinez', 'Cuenca'), ('Den', 'Norske', 'Stats'), ('Norske', 'Stats', 'Oljeselskap'), ('Kokusai', 'Denshin', 'Denwa'), ('Special', 'Drawing', 'Rights'), ('Dar', 'es', 'Salaam'), ('FOLLOWING', 'RAINFALL', 'WAS'), ('Duffour', 'et', 'Igon'), ('Tender', 'Loving', 'Care'), ('CATTLE', 'SLAUGHTER', 'GUESSTIMATES'), ('CAMPBELL', 'RED', 'LAKE'), ('Victor', 'Paz', 'Estenssoro'), ('Carter', 'Hawley', 'Hale'), ('Punta', 'del', 'Este'), ('ELEVATOR', 'LOADING', 'WAITING'), ('TIME', 'JOBLESS', 'CLAIMS'), ('Francaise', 'des', 'Petroles'), ('Public', 'Sector', 'Borrowing'), ('Arturo', 'Hernandez', 'Grisanti'), ('Speaker', 'Jim', 'Wright'), ('carrier', 'Kitty', 'Hawk'), ('Archer', 'Daniels', 'Midland'), ('Corning', 'Glass', 'Works'), ('refined', 'bleached', 'deodorised'), ('Grown', 'Cereals', 'Authority'), ('Commissioner', 'Frans', 'Andriessen'), ('RBD', 'PALM', 'OLEIN'), ('RAINFALL', 'THE', 'FOLLOWING'), ('THE', 'FOLLOWING', 'RAINFALL'), ('Kremlin', 'leader', 'Mikhail'), ('Bankhaus', 'Centrale', 'Credit'), ('SANTA', 'FE', 'SOUTHERN'), ('Director', 'Kobena', 'Erbynn'), ('THOUS', 'BUSHELS', 'SOYBEANS'), ('GETS', 'QUALIFIED', 'AUDIT'), ('Denis', 'Bra', 'Kanon'), ('GHANA', 'COCOA', 'PURCHASES'), ('bleached', 'deodorised', 'palm'), ('leader', 'Mikhail', 'Gorbachev'), ('SLAUGHTER', 'GUESSTIMATES', 'Chicago'), ('de', 'Constructions', 'Telephoniques'), ('DISCOUNT', 'WINDOW', 'BORROWINGS'), ('Nil', 'Nil', 'Nil'), ('Fichtel', 'und', 'Sachs'), ('de', 'Zoete', 'Wedd'), ('Home', 'Grown', 'Cereals')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "#Bigrams\n",
    "finder = BigramCollocationFinder.from_words(nltk.corpus.reuters.words())\n",
    "finder.apply_freq_filter(5)\n",
    "\n",
    "print(\"Best 50 bigrams according to PMI:\", finder.nbest(bigram_measures.pmi, 50))\n",
    "\n",
    "#Trigrams\n",
    "finder = TrigramCollocationFinder.from_words(nltk.corpus.reuters.words())\n",
    "finder.apply_freq_filter(5)\n",
    "\n",
    "print(\"Best 50 bigrams according to PMI:\", finder.nbest(trigram_measures.pmi, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion between diferent data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['John', 'works', 'at', 'OBI', '.']\n",
      "\n",
      "Tagged tokens:  [('John', 'NNP'), ('works', 'VBZ'), ('at', 'IN'), ('OBI', 'NNP'), ('.', '.')]\n",
      "\n",
      "Untagged tokens ['John', 'works', 'at', 'OBI', '.']\n",
      "\n",
      "Tagged tokens to strings: ['John/NNP', 'works/VBZ', 'at/IN', 'OBI/NNP', './.']\n",
      "\n",
      "Tagged tokens from strings to tuples: [('John', 'NNP'), ('works', 'VBZ'), ('at', 'IN'), ('OBI', 'NNP'), ('.', '.')]\n",
      "\n",
      "NER tree: (S (PERSON John/NNP) works/VBZ at/IN (ORGANIZATION OBI/NNP) ./.)\n",
      "\n",
      "IOB tagged tree: [('John', 'NNP', 'B-PERSON'), ('works', 'VBZ', 'O'), ('at', 'IN', 'O'), ('OBI', 'NNP', 'B-ORGANIZATION'), ('.', '.', 'O')]\n",
      "\n",
      "Back to tree: (S (PERSON John/NNP) works/VBZ at/IN (ORGANIZATION OBI/NNP) ./.)\n",
      "\n",
      "Tree as CoNLL string:\n",
      " John NNP B-PERSON\n",
      "works VBZ O\n",
      "at IN O\n",
      "OBI NNP B-ORGANIZATION\n",
      ". . O\n",
      "\n",
      "CoNLL string to tree: (S (PERSON John/NNP) works/VBZ at/IN (ORGANIZATION OBI/NNP) ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tag import untag, str2tuple, tuple2str\n",
    "from nltk.chunk import tree2conllstr, conllstr2tree, conlltags2tree, tree2conlltags\n",
    " \n",
    "text = \"John works at OBI.\"\n",
    " \n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens: \", tokens)\n",
    " \n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(\"\\nTagged tokens: \", tagged_tokens)\n",
    " \n",
    "print(\"\\nUntagged tokens\", untag(tagged_tokens))\n",
    " \n",
    "tagged_tokens = [tuple2str(t) for t in tagged_tokens] \n",
    "print(\"\\nTagged tokens to strings:\", tagged_tokens)\n",
    " \n",
    "tagged_tokens = [str2tuple(t) for t in tagged_tokens]\n",
    "print(\"\\nTagged tokens from strings to tuples:\",  tagged_tokens)\n",
    " \n",
    "ner_tree = ne_chunk(tagged_tokens)\n",
    "print(\"\\nNER tree:\", ner_tree)\n",
    " \n",
    "iob_tagged = tree2conlltags(ner_tree)\n",
    "print(\"\\nIOB tagged tree:\", iob_tagged)\n",
    " \n",
    "ner_tree = conlltags2tree(iob_tagged)\n",
    "print(\"\\nBack to tree:\", ner_tree)\n",
    " \n",
    "tree_str = tree2conllstr(ner_tree)\n",
    "print(\"\\nTree as CoNLL string:\\n\", tree_str)\n",
    " \n",
    "ner_tree = conllstr2tree(tree_str, chunk_types=('PERSON', 'ORGANIZATION'))\n",
    "print(\"\\nCoNLL string to tree:\", ner_tree)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
